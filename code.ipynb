{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #2 \n",
    "\n",
    "TRABZI Mohammed Amine\n",
    "\n",
    "Master 2 Data Sciene \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxF4tX0x_Opc",
    "outputId": "eceb261a-cb7b-45d0-e435-a056f05fadb2"
   },
   "source": [
    "# Downloads & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim \n",
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: wget: not found\n",
      "/bin/sh: 1: unzip: not found\n"
     ]
    }
   ],
   "source": [
    "!wget https://perso.limsi.fr/neveol/TP_ISD2020.zip\n",
    "!unzip TP_ISD2020.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/amine/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/amine/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLvkF1n9_nZp",
    "outputId": "529e2e37-a044-4efb-8275-7731695a478c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FrenchPress_w2v_SKIPGRAM.vec.gz', 'FrenchPress_FastText_CBOW.vec.gz', 'run_ner_conll03.sh', 'embed', 'output', 'ner.py', 'FrenchMed_FastText_CBOW.vec.gz', 'README', 'parsing.py', 'FrenchMed_w2v_SKIPGRAM.vec.gz', 'run-ner-q6-20-half.sh', 'neuronlp2', '.run-ner-q6-20-half.sh.swo', 'quaero-100-demi.json', 'FrenchPress_w2v_CBOW.vec.gz', 'TP_ISD2020.zip', 'TRABZI_code_TP1.ipynb', 'pos_tagging.py', 'QUAERO', 'FrenchMed_w2v_CBOW.vec.gz', 'FrenchMed_w2v_CBOW.vec', 'QUAERO_FrenchPress-w2v.vec.gz', '.ipynb_checkpoints'] \n",
      "\n",
      "FrenchPress_FastText_CBOW.vec.gz ..... $PATH =  /home/amine/tp2/FrenchPress_FastText_CBOW.vec.gz \n",
      "\n",
      "output ..... $PATH =  /home/amine/tp2/output\n"
     ]
    }
   ],
   "source": [
    "directory=os.listdir()\n",
    "print(directory,\"\\n\")\n",
    "dir_PATH=os.path.abspath(directory[1])\n",
    "print(directory[1],\"..... $PATH = \",dir_PATH,\"\\n\")\n",
    "dir_PATH=os.path.abspath(directory[4])\n",
    "print(directory[4],\"..... $PATH = \",dir_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(sentence):\n",
    "    return [word.lower() for word in nltk.word_tokenize(sentence) if word.isalnum()]\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [filter_text(sentence) for sentence in nltk.sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUAERO_FrenchMed_traindev.ospl\n",
      "QUAERO_FrenchPress_traindev.ospl\n",
      "\n",
      "number of sentences in sentences_FrenchPress: 3000\n",
      "number of tokens in sentences_FrenchPress: 43067 \n",
      "\n",
      "number of sentences in sentences_FrenchMed: 43541\n",
      "number of tokens in sentences_FrenchMed: 1113538\n"
     ]
    }
   ],
   "source": [
    "sentences_FrenchPress=[]\n",
    "sentences_FrenchMed=[]\n",
    "\n",
    "text_=[\"QUAERO_FrenchMed_traindev.ospl\",\"QUAERO_FrenchPress_traindev.ospl\"]\n",
    "for root,directory,files in os.walk(\"QUAERO\"):\n",
    "    for filename in files:\n",
    "        if filename in text_[0]:\n",
    "            print(filename)\n",
    "            with open(os.path.join(root,filename),'r') as rf:\n",
    "                text=rf.read()\n",
    "                sentences_FrenchPress.extend(tokenize_text(text))\n",
    "                num_words_FrenchPress=filter_text(text)#nltk.word_tokenize(text)\n",
    "        elif filename in text_[1]:\n",
    "            print(filename)\n",
    "            with open(os.path.join(root,filename),'r') as rf:\n",
    "                text=rf.read()\n",
    "                sentences_FrenchMed.extend(tokenize_text(text))\n",
    "                num_words_FrenchMed=filter_text(text) #nltk.word_tokenize(text)\n",
    "                \n",
    "print(\"\\nnumber of sentences in sentences_FrenchPress:\",len(sentences_FrenchPress))\n",
    "print(\"number of tokens in sentences_FrenchPress:\",len(num_words_FrenchPress),\"\\n\")\n",
    "\n",
    "print(\"number of sentences in sentences_FrenchMed:\",len(sentences_FrenchMed))\n",
    "print(\"number of tokens in sentences_FrenchMed:\",len(num_words_FrenchMed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named  Entity Recognition , using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_txt = nlp(text[0:500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Patricia, Martin)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Nicolas, Stoufflet)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(France, Inter)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(7)</td>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Simon, Tivolle)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3809</th>\n",
       "      <td>(Jean, -, Marie, Le, Pen)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3810</th>\n",
       "      <td>(Bruno, Mégret)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>(Front, National)</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>(Charles, Millon)</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3813</th>\n",
       "      <td>(Rhône, /, Alpes)</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3814 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Entities    Labels\n",
       "0            (Patricia, Martin)    PERSON\n",
       "1          (Nicolas, Stoufflet)    PERSON\n",
       "2               (France, Inter)    PERSON\n",
       "3                           (7)  CARDINAL\n",
       "4              (Simon, Tivolle)    PERSON\n",
       "...                         ...       ...\n",
       "3809  (Jean, -, Marie, Le, Pen)    PERSON\n",
       "3810            (Bruno, Mégret)    PERSON\n",
       "3811          (Front, National)       ORG\n",
       "3812          (Charles, Millon)    PERSON\n",
       "3813          (Rhône, /, Alpes)       ORG\n",
       "\n",
       "[3814 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = []\n",
    "entities_list = []\n",
    "\n",
    "for ent in nlp_txt.ents:\n",
    "    labels_list.append(ent.label_)\n",
    "    entities_list.append(ent)\n",
    "df = pd.DataFrame({'Entities':entities_list,'Labels':labels_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #1 : word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1u7HOib_qEl",
    "outputId": "7e9244b1-6162-4a61-b829-f2f08cdb7617",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=9104, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=9104, size=100, alpha=0.025)\n",
      "FastText(vocab=9104, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=39654, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=39654, size=100, alpha=0.025)\n",
      "FastText(vocab=39654, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#sentences_FrenchMed = LineSentence(datapath('/home/amine/tp2/QUAERO_FrenchMed/QUAERO_FrenchMed_traindev.ospl'))\n",
    "#sentences_FrenchPress = LineSentence(datapath('/home/amine/tp2/QUAERO_FrenchPress/QUAERO_FrenchPress_traindev.ospl'))\n",
    "\n",
    "# (1) #French_Med\n",
    "\n",
    "#French_Med\n",
    "FrenchMed_w2v_CBOW = Word2Vec(sentences_FrenchMed, size=100, min_count=1, sg=0)\n",
    "print(FrenchMed_w2v_CBOW)\n",
    "\n",
    "FrenchMed_w2v_SKIPGRAM = Word2Vec(sentences_FrenchMed, size=100, min_count=1, sg=1)\n",
    "print(FrenchMed_w2v_SKIPGRAM)\n",
    "\n",
    "FrenchMed_FastText_CBOW = FastText(sentences_FrenchMed, size=100, min_count=1, sg=0)\n",
    "print(FrenchMed_FastText_CBOW)\n",
    "\n",
    "# (2) French_Press\n",
    "\n",
    "FrenchPress_w2v_CBOW = Word2Vec(sentences_FrenchPress, size=100, min_count=1, sg=0)\n",
    "print(FrenchPress_w2v_CBOW)\n",
    "\n",
    "FrenchPress_w2v_SKIPGRAM = Word2Vec(sentences_FrenchPress, size=100, min_count=1, sg=1)\n",
    "print(FrenchPress_w2v_SKIPGRAM)\n",
    "\n",
    "FrenchPress_FastText_CBOW = FastText(sentences_FrenchPress, size=100, min_count=1, sg=0)\n",
    "print(FrenchPress_FastText_CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FrenchMed_w2v_CBOW.wv.save_word2vec_format('FrenchMed_w2v_CBOW.vec.gz', binary=False)\n",
    "\n",
    "FrenchMed_w2v_CBOW.wv.save_word2vec_format('FrenchMed_w2v_CBOW.vec.gz', binary=False)\n",
    "FrenchMed_w2v_SKIPGRAM.wv.save_word2vec_format('FrenchMed_w2v_SKIPGRAM.vec.gz', binary=False)\n",
    "FrenchMed_FastText_CBOW.wv.save_word2vec_format('FrenchMed_FastText_CBOW.vec.gz', binary=False)\n",
    "FrenchPress_w2v_CBOW.wv.save_word2vec_format('FrenchPress_w2v_CBOW.vec.gz', binary=False)\n",
    "FrenchPress_w2v_SKIPGRAM.wv.save_word2vec_format('FrenchPress_w2v_SKIPGRAM.vec.gz', binary=False)\n",
    "FrenchPress_FastText_CBOW.wv.save_word2vec_format('FrenchPress_FastText_CBOW.vec.gz', binary=False)\n",
    "\n",
    "\n",
    "'''FrenchMed_w2v_CBOW.save('FrenchMed_w2v_CBOW.vec.gz')\n",
    "FrenchMed_w2v_SKIPGRAM.save('FrenchMed_w2v_SKIPGRAM.vec.gz')\n",
    "#FrenchMed_FastText_CBOW.save('FrenchMed_FastText_CBOW.p')\n",
    "FrenchPress_w2v_CBOW.save('FrenchPress_w2v_CBOW.vec.gz')\n",
    "FrenchPress_w2v_SKIPGRAM.save('FrenchPress_w2v_SKIPGRAM.vec.gz')\n",
    "#FrenchPress_FastText_CBOW.save('FrenchPress_FastText_CBOW.p')'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcOHZa-fg2tm",
    "outputId": "a6ed6680-ee51-4026-8351-61ec716d5ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> French_Med :\n",
      "\n",
      "\n",
      " patient :\n",
      "w2v CBOW  :  ['d', 'du', 'un', 'des', 'le', 'et', 'en', 'à', 'aux', 'l']\n",
      "w2v SKIPGRAM  :  ['symptômes', 'lemp', 'grossesse', 'également', 'refludan', 'qui', 'cette', 'avant', 'devra', 'recommandé']\n",
      "fast text CBOW  :  ['patiente', 'conscients', 'patients', 'appartient', 'présentaient', 'aient', 'parvient', 'maintient', 'avaient', 'soient']\n",
      "\n",
      " treatment :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['lentement', 'localement', 'seulement', 'traitment', 'décollement', 'généralement', 'enregistrement', 'avancement', 'également', 'hautement']\n",
      "\n",
      " disease :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['telle', 'alvéolaire', 'épidémiologie', 'presse', 'constrictive', 'une', 'perivasculaire', 'dispose', 'prostate', 'vasculaire']\n",
      "\n",
      " solution :\n",
      "w2v CBOW  :  ['ml', 'flacon', 'perfusion', 'mg', 'contient', '2', 'pour', '1', '5', 'chaque']\n",
      "w2v SKIPGRAM  :  ['contient', 'flacon', '100', 'chaque', 'g', 'μ', '2', '1', '20', '300']\n",
      "fast text CBOW  :  ['dissolution', 'évolution', 'evolution', 'dilution', 'pollution', 'élocution', 'solutions', 'exécution', 'substitution', 'institution']\n",
      "\n",
      " yellow :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['échec', 'excrétés', 'poplité', 'ratte', 'voie', 'hospitalisés', 'petit', 'générés', 'polysorbate', 'pèsent']\n",
      "\n",
      "\n",
      "\n",
      "--> French_Press :\n",
      "\n",
      "\n",
      " patient :\n",
      "w2v CBOW  :  ['effort', 'faux', 'airbus', 'délit', 'témoignait', 'potentiel', 'digne', 'edf', 'mesureur', 'effectuer']\n",
      "w2v SKIPGRAM  :  ['miracle', 'embauche', 'invalider', 'motivation', 'journalistique', 'flic', 'identification', 'circonstance', 'ordinateur', 'copier']\n",
      "fast text CBOW  :  ['patientent', 'soutient', 'détient', 'impatient', 'contient', 'coefficient', 'convient', 'prient', 'initient', 'abstient']\n",
      "\n",
      " treatment :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['sédiment', 'ment', 'fument', 'clément', 'joliment', 'rythment', 'dorment', 'ciment', 'calment', 'sentiment']\n",
      "\n",
      " disease :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['dise', 'diseuse', 'dispose', 'réalise', 'diabolise', 'diffuse', 'sensibilise', 'cruise', 'repose', 'plaise']\n",
      "\n",
      " solution :\n",
      "w2v CBOW  :  ['forme', 'règle', 'réponse', 'catastrophe', 'espèce', 'démarche', 'carte', 'seule', 'possibilité', 'dimension']\n",
      "w2v SKIPGRAM  :  ['règle', 'alternative', 'puissance', 'garantie', 'consensuelle', 'unie', 'difficulté', 'démocratie', 'crédibilité', 'morale']\n",
      "fast text CBOW  :  ['révolution', 'résolution', 'évolution', 'pollution', 'dilution', 'dissolution', 'caution', 'potion', 'ponctuation', 'formulation']\n",
      "\n",
      " yellow :\n",
      "w2v CBOW  :  [None]\n",
      "w2v SKIPGRAM  :  [None]\n",
      "fast text CBOW  :  ['shell', 'well', 'bellemar', 'mello', 'belley', 'melloul', 'kelly', 'bellynck', 'kella', 'sigonella']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_word_list=[\"patient\", \"treatment\", \"disease\", \"solution\", \"yellow\"]\n",
    "\n",
    "approaches=[\"w2v CBOW\", \"w2v SKIPGRAM\", \"fast text CBOW\"]\n",
    "\n",
    "corpora=[\"French_Med\",\"French_Press\"]\n",
    "\n",
    "\n",
    "models_French_Med = [FrenchMed_w2v_CBOW, FrenchMed_w2v_SKIPGRAM, FrenchMed_FastText_CBOW ]\n",
    "models_French_Press = [FrenchPress_w2v_CBOW, FrenchPress_w2v_SKIPGRAM , FrenchPress_FastText_CBOW]\n",
    "\n",
    "models=[]\n",
    "models.append(models_French_Med)\n",
    "models.append(models_French_Press)\n",
    "\n",
    "m=0\n",
    "for k in corpora:\n",
    "    print(\"\\n-->\",k,\":\\n\")\n",
    "    for i in candidate_word_list:\n",
    "        a=0\n",
    "        print(\"\\n\",i,\":\")\n",
    "        for j in models[m]:\n",
    "            try:\n",
    "                list_embeddings = [embeddings[0] for embeddings in j.wv.most_similar(i)]\n",
    "                print(approaches[a],\" : \",list_embeddings)\n",
    "            except:\n",
    "                list_embeddings=[]\n",
    "                print(approaches[a],\" : \",\"[None]\")\n",
    "            a+=1\n",
    "    m+=1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtZa1GJ5aqJ2",
    "outputId": "698ec1f2-e8a3-489b-d83d-5581b6ce9537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solution', 'patient']\n"
     ]
    }
   ],
   "source": [
    "# Looking for availability of candidate_word_list in the datasets\n",
    "search=[]\n",
    "number_of_sets=2\n",
    "for _ in range(number_of_sets):\n",
    "\n",
    "  for j in models_French_Med:\n",
    "    for i in j.wv.vocab:\n",
    "      if i in candidate_word_list:\n",
    "        if i not in search:\n",
    "          search.append(i)\n",
    "\n",
    "  for j in models_French_Press:\n",
    "    for i in j.wv.vocab:\n",
    "      if i in candidate_word_list:\n",
    "        if i not in search:\n",
    "          search.append(i)\n",
    "          \n",
    "print(search)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
